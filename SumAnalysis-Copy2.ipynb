{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 金融风控学习赛\n",
    "## 1.读取数据并进行观察（略）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data shape: (800000, 47)\n",
      "TestA data shape: (200000, 46)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "from numpy import inf\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import norm\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy import stats\n",
    "plt.style.use('fivethirtyeight')\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "%matplotlib inline\n",
    "\n",
    "#读取基础数据阶段\n",
    "Train_data = pd.read_csv('train.csv',encoding='utf-8',delimiter=',')\n",
    "Test_data =pd.read_csv('testA.csv',encoding='utf-8',delimiter=',' )\n",
    "n_train = len(Train_data)\n",
    "print('Train data shape:',Train_data.shape)\n",
    "print('TestA data shape:',Test_data.shape)\n",
    "\n",
    "import os\n",
    "path = os.path.abspath(os.path.dirname(os.getcwd()) + os.path.sep + \".\")\n",
    "datapath = path + '/2.Risk_of_Loan/Processdata/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split,GridSearchCV,KFold\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.neighbors import KNeighborsClassifier,KNeighborsRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier,RandomForestRegressor\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler,MinMaxScaler\n",
    "\n",
    "from sklearn.metrics import accuracy_score,mean_squared_error,roc_auc_score\n",
    "\n",
    "from lightgbm import LGBMClassifier\n",
    "from catboost import CatBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 用来减少内存使用的方程\n",
    "def reduce_mem_usage(df):\n",
    "    \"\"\" iterate through all the columns of a dataframe and modify the data type\n",
    "        to reduce memory usage.        \n",
    "    \"\"\"\n",
    "    start_mem = df.memory_usage().sum()  / 1024**2\n",
    "    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n",
    "    \n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtype\n",
    "        \n",
    "        if col_type != object:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "        else:\n",
    "            df[col] = df[col].astype('category')\n",
    "\n",
    "    end_mem = df.memory_usage().sum()  / 1024**2\n",
    "    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n",
    "    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All Features in raw data are: (47,) \n",
      " Index(['id', 'loanAmnt', 'term', 'interestRate', 'installment', 'grade',\n",
      "       'subGrade', 'employmentTitle', 'employmentLength', 'homeOwnership',\n",
      "       'annualIncome', 'verificationStatus', 'issueDate', 'isDefault',\n",
      "       'purpose', 'postCode', 'regionCode', 'dti', 'delinquency_2years',\n",
      "       'ficoRangeLow', 'ficoRangeHigh', 'openAcc', 'pubRec',\n",
      "       'pubRecBankruptcies', 'revolBal', 'revolUtil', 'totalAcc',\n",
      "       'initialListStatus', 'applicationType', 'earliesCreditLine', 'title',\n",
      "       'policyCode', 'n0', 'n1', 'n2', 'n3', 'n4', 'n5', 'n6', 'n7', 'n8',\n",
      "       'n9', 'n10', 'n11', 'n12', 'n13', 'n14'],\n",
      "      dtype='object')\n",
      "Useless Features are: ['isDefault', 'policyCode']\n",
      "Useless Features are already dropped... (45,)\n"
     ]
    }
   ],
   "source": [
    "All_Y = Train_data['isDefault']\n",
    "All_Y.to_csv(datapath+'Y_Final.csv',index=False)\n",
    "\n",
    "All_data = pd.concat([Train_data,Test_data]).reset_index(drop=True)\n",
    "print('All Features in raw data are:',All_data.columns.shape,'\\n',All_data.columns)\n",
    "Useless_Feature='isDefault/policyCode' \n",
    "Useless_F=Useless_Feature.split('/')\n",
    "print('Useless Features are:',Useless_F)\n",
    "All_data = All_data.drop(Useless_F,axis=1)\n",
    "print('Useless Features are already dropped...',All_data.columns.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "#用于快速计算LGB未调参状态下的AUC值以便比较\n",
    "def fast_test_lgbauc(All_data): \n",
    "    All_data =All_data.drop(['id','isDefault'],axis=1)\n",
    "    All_train =  All_data[:800000] \n",
    "    All_test = All_data[800000:] \n",
    "    print('All_train size',All_train.shape,'All_test size',All_test.shape)\n",
    "    del All_data\n",
    "\n",
    "    X_Train, X_Test, y_Train, y_Test = train_test_split(All_train, All_Y, test_size=0.2, random_state=233)\n",
    "\n",
    "    LGB = LGBMClassifier(random_state=9,n_jobs = -1).fit(X_Train,y_Train)\n",
    "    print(\"LGB AUC Train score: %.4f  \" % roc_auc_score(y_Train,LGB.predict_proba(X_Train)[:,1]),end='')\n",
    "    print(\"LGB AUC Test score: %.4f\" % roc_auc_score(y_Test,LGB.predict_proba(X_Test)[:,1]))\n",
    "    importances = LGB.feature_importances_\n",
    "    Featurename = LGB.feature_name_\n",
    "    feature_importance = pd.DataFrame({'feature_name':Featurename,'importance':importances})\n",
    "    return feature_importance\n",
    "\n",
    "#该函数用于计算缺失率\n",
    "def calculate_missratio(Dataset,n=5):\n",
    "    All_data = Dataset\n",
    "    total = All_data.isnull().sum().sort_values(ascending = False)\n",
    "    percent = (All_data.isnull().sum()/All_data.isnull().count()).sort_values(ascending = False)\n",
    "    missing_data = pd.concat([total,percent],axis=1,keys=['total','Percent'])\n",
    "    print(missing_data.head(n))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 采用随机森林填充相关缺失值"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 对非int/float特征进行一些处理方便进行填充\n",
    "主要填充的参数如下\n",
    "~~~\n",
    "grade_replace = {'A':1,'B':2,'C':3,'D':4,'E':5,'F':6,'G':7}\n",
    "subGrade_replace = {'A1':1,'A2':2,'A3':3,'A4':4,'A5':5,'B1':6,'B2':7,'B3':8,'B4':9,'B5':10,\n",
    "                    'C1':11,'C2':12,'C3':13,'C4':14,'C5':15,'D1':16,'D2':17,'D3':18,'D4':19,'D5':20,\n",
    "                    'E1':21,'E2':22,'E3':23,'E4':24,'E5':25,'F1':26,'F2':27,'F3':28,'F4':29,'F5':30,\n",
    "                    'G1':31,'G2':32,'G3':33,'G4':34,'G5':35}\n",
    "year_replace = {'< 1 year' :0, '1 year':1, '2 years':2, '3 years':3, '4 years':4, '5 years':5, \n",
    "                 '6 years':6, '7 years':7,  '8 years':8, '9 years':9, '10+ years' :10}\n",
    "                 \n",
    "                 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classdata Processing......:\n",
      "Example of data\n",
      "   grade subGrade employmentLength\n",
      "0     E       E2          2 years\n",
      "1     D       D2          5 years\n",
      "Classdata Processing finished\n",
      "Example of data\n",
      "    grade  subGrade  employmentLength\n",
      "0      5        22               2.0\n",
      "1      4        17               5.0\n"
     ]
    }
   ],
   "source": [
    "print('Classdata Processing......:')\n",
    "print('Example of data\\n',All_data[['grade','subGrade','employmentLength']].head(2))\n",
    "\n",
    "grade_replace = {'A':1,'B':2,'C':3,'D':4,'E':5,'F':6,'G':7}\n",
    "subGrade_replace = {'A1':1,'A2':2,'A3':3,'A4':4,'A5':5,'B1':6,'B2':7,'B3':8,'B4':9,'B5':10,\n",
    "                    'C1':11,'C2':12,'C3':13,'C4':14,'C5':15,'D1':16,'D2':17,'D3':18,'D4':19,'D5':20,\n",
    "                    'E1':21,'E2':22,'E3':23,'E4':24,'E5':25,'F1':26,'F2':27,'F3':28,'F4':29,'F5':30,\n",
    "                    'G1':31,'G2':32,'G3':33,'G4':34,'G5':35}\n",
    "year_replace = {'< 1 year' :0, '1 year':1, '2 years':2, '3 years':3, '4 years':4, '5 years':5, \n",
    "                 '6 years':6, '7 years':7,  '8 years':8, '9 years':9, '10+ years' :10}\n",
    "\n",
    "All_data['grade'] = All_data['grade'].replace(grade_replace)     \n",
    "All_data['subGrade'] = All_data['subGrade'].replace(subGrade_replace)     \n",
    "All_data['employmentLength'] = All_data['employmentLength'].replace(year_replace)     \n",
    "print('Classdata Processing finished')\n",
    "print('Example of data\\n',All_data[['grade','subGrade','employmentLength']].head(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 对日期特征进行处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFormat Processing......:\n",
      "DataFormat Features are already changed... (47,)\n",
      "DataFormat Features are already added... (52,)\n",
      "DataFormat Features temp are already removed... (48,)\n"
     ]
    }
   ],
   "source": [
    "print('DataFormat Processing......:')\n",
    "All_data['issueDate_refmt']=pd.to_datetime(All_data['issueDate'],format ='%Y-%m-%d')\n",
    "All_data['earliesCreditLine_refmt']=pd.to_datetime(All_data['earliesCreditLine'],format ='%b-%Y')\n",
    "print('DataFormat Features are already changed...',All_data.columns.shape)\n",
    "\n",
    "All_data['issueDate_month']=All_data['issueDate_refmt'].dt.month\n",
    "All_data['issueDate_year']=All_data['issueDate_refmt'].dt.year\n",
    "All_data['earliesCreditLine_month']=All_data['earliesCreditLine_refmt'].dt.month\n",
    "All_data['earliesCreditLine_year']=All_data['earliesCreditLine_refmt'].dt.month\n",
    "All_data['Credit_range_months']=round((All_data['issueDate_refmt']-All_data['earliesCreditLine_refmt']).dt.days/30,1)\n",
    "\n",
    "print('DataFormat Features are already added...',All_data.columns.shape)\n",
    "\n",
    "droplisttext='issueDate/earliesCreditLine/issueDate_refmt/earliesCreditLine_refmt'\n",
    "droplist = droplisttext.split('/')\n",
    "All_data = All_data.drop(droplist,axis=1)\n",
    "print('DataFormat Features temp are already removed...',All_data.columns.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "All_data.to_csv(datapath+'All_data_NewStep1.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "无任何处理情况下基础LGB的AUC值如下\n",
    "```\n",
    "All_train size (800000, 48) All_test size (200000, 48)\n",
    "LGB AUC Train score: 0.7398  LGB AUC Test score: 0.7296\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 开始将n0-n14的匿名变量用进行稍微分桶以便提高Random填充精度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "All_data = pd.read_csv(datapath + 'All_data_NewStep1.csv',encoding='utf-8',delimiter=',')\n",
    "All_Y = pd.read_csv(datapath + 'Y_Final.csv',encoding='utf-8',delimiter=',')\n",
    "All_data['isDefault'] = All_Y\n",
    "Normal_Columns=set(All_data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "All_data['n1'] = np.floor_divide(All_data['n1'], 3)\n",
    "All_data['n2'] = np.floor_divide(All_data['n2'], 5)\n",
    "All_data['n3'] = np.floor_divide(All_data['n3'], 4)\n",
    "All_data['n4'] = np.floor_divide(All_data['n4'], 5)\n",
    "All_data['n5'] = np.floor_divide(All_data['n5'], 6)\n",
    "All_data['n6'] = np.floor_divide(All_data['n6'], 10)\n",
    "All_data['n7'] = np.floor_divide(All_data['n7'], 10)\n",
    "All_data['n8'] = np.floor_divide(All_data['n8'], 10)\n",
    "All_data['n9'] = np.floor_divide(All_data['n9'], 2)\n",
    "All_data['n10'] = np.floor_divide(All_data['n10'], 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "All_data['n1'].loc[All_data['n1']>7] = 7\n",
    "All_data['n2'].loc[All_data['n2']>7] = 7\n",
    "All_data['n3'].loc[All_data['n3']>8] = 8\n",
    "All_data['n4'].loc[All_data['n4']>7] = 7\n",
    "All_data['n5'].loc[All_data['n5']>9] = 9\n",
    "All_data['n6'].loc[All_data['n6']>9] = 9\n",
    "All_data['n7'].loc[All_data['n7']>5] = 5\n",
    "All_data['n8'].loc[All_data['n8']>9] = 9\n",
    "All_data['n9'].loc[All_data['n9']>12] = 12\n",
    "All_data['n10'].loc[All_data['n10']>11] = 11\n",
    "All_data['n11'] = All_data['n11'].replace({3:2,4:2})\n",
    "All_data['n12'] = All_data['n12'].replace({3:2,4:2})\n",
    "All_data['n13'].loc[All_data['n13']>10] = 10\n",
    "All_data['n14'].loc[All_data['n14']>10] = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 填充缺失值少于100的特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fill Data with only 1 NaN......:\n",
      "Feature Filling Target:\n",
      " ['employmentTitle', 'title', 'postCode']\n",
      "Feature: employmentTitle is already filled by mode\n",
      "Feature: title is already filled by mode\n",
      "Feature: postCode is already filled by mode\n"
     ]
    }
   ],
   "source": [
    "print('Fill Data with only 1 NaN......:')\n",
    "FeatureFillTar = 'employmentTitle/title/postCode'\n",
    "FeatureFill=FeatureFillTar.split('/')\n",
    "print('Feature Filling Target:\\n',FeatureFill)\n",
    "for var in FeatureFill:\n",
    "    fillvalue = float(All_data[var].mode())\n",
    "    All_data[var] = All_data[var].fillna(fillvalue)\n",
    "    print('Feature:',var,'is already filled by mode')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "All_data.to_csv(datapath+'All_data_NewStep2.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5用随机森林填充缺失值(离散值)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The features without null data: (29,) \n",
      " Index(['id', 'loanAmnt', 'term', 'interestRate', 'installment', 'grade',\n",
      "       'subGrade', 'employmentTitle', 'homeOwnership', 'annualIncome',\n",
      "       'verificationStatus', 'purpose', 'postCode', 'regionCode',\n",
      "       'delinquency_2years', 'ficoRangeLow', 'ficoRangeHigh', 'openAcc',\n",
      "       'pubRec', 'revolBal', 'totalAcc', 'initialListStatus',\n",
      "       'applicationType', 'title', 'issueDate_month', 'issueDate_year',\n",
      "       'earliesCreditLine_month', 'earliesCreditLine_year',\n",
      "       'Credit_range_months'],\n",
      "      dtype='object')\n",
      "The features have null data: (20,) \n",
      " Index(['employmentLength', 'dti', 'pubRecBankruptcies', 'revolUtil', 'n0',\n",
      "       'n1', 'n2', 'n3', 'n4', 'n5', 'n6', 'n7', 'n8', 'n9', 'n10', 'n11',\n",
      "       'n12', 'n13', 'n14', 'isDefault'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "All_data = pd.read_csv(datapath + 'All_data_NewStep2.csv',encoding='utf-8',delimiter=',')\n",
    "\n",
    "Full_feature =  All_data.describe().T['count'][All_data.describe().T['count'].values == All_data.shape[0]].index\n",
    "print('The features without null data:',Full_feature.shape,'\\n',Full_feature)\n",
    "Miss_feature =  All_data.describe().T['count'][All_data.describe().T['count'].values != All_data.shape[0]].index\n",
    "print('The features have null data:',Miss_feature.shape,'\\n',Miss_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------\n",
      "Feature Filling Target: 17 \n",
      " ['n0', 'n1', 'n2', 'n3', 'n4', 'n5', 'n6', 'n7', 'n8', 'n9', 'n10', 'n11', 'n12', 'n13', 'n14', 'employmentLength', 'pubRecBankruptcies']\n",
      "----------------------------------------------------------------------------\n",
      "Feature Used for filling: 28 \n",
      " ['loanAmnt', 'term', 'interestRate', 'installment', 'grade', 'subGrade', 'employmentTitle', 'homeOwnership', 'annualIncome', 'verificationStatus', 'purpose', 'postCode', 'regionCode', 'delinquency_2years', 'ficoRangeLow', 'ficoRangeHigh', 'openAcc', 'pubRec', 'revolBal', 'totalAcc', 'initialListStatus', 'applicationType', 'title', 'issueDate_month', 'issueDate_year', 'earliesCreditLine_month', 'earliesCreditLine_year', 'Credit_range_months']\n",
      "----------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "Y_FeatureFillClassstr = 'n0/n1/n2/n3/n4/n5/n6/n7/n8/n9/n10/n11/n12/n13/n14/employmentLength/pubRecBankruptcies'\n",
    "Y_FeatureFillClass=Y_FeatureFillClassstr.split('/')\n",
    "X_FeatureFill = [c for c in Full_feature if c not in (Y_FeatureFillClass+['id'])]\n",
    "print('----------------------------------------------------------------------------')\n",
    "print('Feature Filling Target:',len(Y_FeatureFillClass),'\\n',Y_FeatureFillClass)\n",
    "print('----------------------------------------------------------------------------')\n",
    "print('Feature Used for filling:',len(X_FeatureFill),'\\n',X_FeatureFill)\n",
    "print('----------------------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data size before divide the Nan Data: (1000000, 45)\n",
      "----------------------------------------------------------------------------\n",
      "Data size after divide the Nan Data: (858145, 45)\n",
      "The rows which have Nan Data: (141855, 45)\n"
     ]
    }
   ],
   "source": [
    "All_data_Full = All_data[X_FeatureFill + Y_FeatureFillClass]\n",
    "\n",
    "print('Data size before divide the Nan Data:',All_data_Full.shape)\n",
    "print('----------------------------------------------------------------------------')\n",
    "\n",
    "All_data_Null = All_data_Full[All_data_Full.isnull().T.any()]\n",
    "All_data_Full = All_data_Full.dropna(axis = 0)\n",
    "\n",
    "print('Data size after divide the Nan Data:',All_data_Full.shape)\n",
    "print('The rows which have Nan Data:',All_data_Null.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full Feature rows shape: (858145, 28) (858145, 17)\n",
      "Number of Train_dataset (686516, 28) (686516, 17)\n",
      "Number of Test_dataset (171629, 28) (171629, 17)\n",
      "Random Forest Filling processing start...\n"
     ]
    }
   ],
   "source": [
    "X_FillTarget=All_data_Full[X_FeatureFill]\n",
    "y_FillTarget=All_data_Full[Y_FeatureFillClass]\n",
    "print('Full Feature rows shape:',X_FillTarget.shape,y_FillTarget.shape)\n",
    "X_Train, X_Test, y_Train, y_Test = train_test_split(X_FillTarget,y_FillTarget,\n",
    "                                                    test_size = 0.2,\n",
    "                                                    random_state = 233)\n",
    "print('Number of Train_dataset',X_Train.shape,y_Train.shape)\n",
    "print('Number of Test_dataset',X_Test.shape,y_Test.shape)\n",
    "print('Random Forest Filling processing start...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Target Feature now is: n0\n",
      "Wall time: 11.8 s\n",
      "Rf filling acc score: 0.699  Rf filling acc score: 0.695\n",
      "----------------------------------------------------------------------------\n",
      "The Target Feature now is: n1\n",
      "Wall time: 3.13 s\n",
      "Rf filling acc score: 0.609  Rf filling acc score: 0.606\n",
      "----------------------------------------------------------------------------\n",
      "The Target Feature now is: n2\n",
      "Wall time: 3.22 s\n",
      "Rf filling acc score: 0.705  Rf filling acc score: 0.701\n",
      "----------------------------------------------------------------------------\n",
      "The Target Feature now is: n3\n",
      "Wall time: 3.44 s\n",
      "Rf filling acc score: 0.649  Rf filling acc score: 0.647\n",
      "----------------------------------------------------------------------------\n",
      "The Target Feature now is: n4\n",
      "Wall time: 3.39 s\n",
      "Rf filling acc score: 0.702  Rf filling acc score: 0.702\n",
      "----------------------------------------------------------------------------\n",
      "The Target Feature now is: n5\n",
      "Wall time: 3.98 s\n",
      "Rf filling acc score: 0.588  Rf filling acc score: 0.587\n",
      "----------------------------------------------------------------------------\n",
      "The Target Feature now is: n6\n",
      "Wall time: 4.12 s\n",
      "Rf filling acc score: 0.713  Rf filling acc score: 0.710\n",
      "----------------------------------------------------------------------------\n",
      "The Target Feature now is: n7\n",
      "Wall time: 2.8 s\n",
      "Rf filling acc score: 0.851  Rf filling acc score: 0.851\n",
      "----------------------------------------------------------------------------\n",
      "The Target Feature now is: n8\n",
      "Wall time: 3.62 s\n",
      "Rf filling acc score: 0.669  Rf filling acc score: 0.667\n",
      "----------------------------------------------------------------------------\n",
      "The Target Feature now is: n9\n",
      "Wall time: 5.06 s\n",
      "Rf filling acc score: 0.438  Rf filling acc score: 0.434\n",
      "----------------------------------------------------------------------------\n",
      "The Target Feature now is: n10\n",
      "Wall time: 3.53 s\n",
      "Rf filling acc score: 0.986  Rf filling acc score: 0.985\n",
      "----------------------------------------------------------------------------\n",
      "The Target Feature now is: n11\n",
      "Wall time: 1.53 s\n",
      "Rf filling acc score: 0.997  Rf filling acc score: 0.996\n",
      "----------------------------------------------------------------------------\n",
      "The Target Feature now is: n12\n",
      "Wall time: 1.49 s\n",
      "Rf filling acc score: 0.996  Rf filling acc score: 0.996\n",
      "----------------------------------------------------------------------------\n",
      "The Target Feature now is: n13\n",
      "Wall time: 2.98 s\n",
      "Rf filling acc score: 0.938  Rf filling acc score: 0.938\n",
      "----------------------------------------------------------------------------\n",
      "The Target Feature now is: n14\n",
      "Wall time: 5.2 s\n",
      "Rf filling acc score: 0.292  Rf filling acc score: 0.287\n",
      "----------------------------------------------------------------------------\n",
      "The Target Feature now is: employmentLength\n",
      "Wall time: 5.12 s\n",
      "Rf filling acc score: 0.365  Rf filling acc score: 0.367\n",
      "----------------------------------------------------------------------------\n",
      "The Target Feature now is: pubRecBankruptcies\n",
      "Wall time: 3.86 s\n",
      "Rf filling acc score: 0.951  Rf filling acc score: 0.951\n",
      "----------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for var in Y_FeatureFillClass:\n",
    "    print('The Target Feature now is:',var)\n",
    "#------------------------Key code for Filling Model---------------------------#       \n",
    "    Lgb = LGBMClassifier(n_estimators=15,random_state =9,n_jobs=-1)\n",
    "    %time Lgb.fit(X_Train,y_Train[var])\n",
    "    print(\"Rf filling acc score: %.3f  \" % accuracy_score(y_Train[var],Lgb.predict(X_Train)),end=\"\")\n",
    "    print(\"Rf filling acc score: %.3f\" % accuracy_score(y_Test[var],Lgb.predict(X_Test)))\n",
    "    print('----------------------------------------------------------------------------')\n",
    "    Fill_Temp_values = Lgb.predict(All_data_Null[All_data_Null[var].isnull().values==True][X_FeatureFill])\n",
    "#------------------------Key code for Filling Model---------------------------#     \n",
    "    Fill_Temp_indexs = All_data_Null[All_data_Null[var].isnull().values==True].index\n",
    "    All_data_Null[var][Fill_Temp_indexs] = Fill_Temp_values \n",
    "    del Lgb,Fill_Temp_values,Fill_Temp_indexs\n",
    "    \n",
    "Fill_data = pd.concat([All_data_Null,All_data_Full],axis=0)\n",
    "All_data[Y_FeatureFillClass] = Fill_data[Y_FeatureFillClass]\n",
    "del Fill_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6用随机森林填充缺失值(连续值)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------\n",
      "Feature Filling Target: 2 \n",
      " ['dti', 'revolUtil']\n",
      "----------------------------------------------------------------------------\n",
      "Feature Used for filling: 45 \n",
      " ['loanAmnt', 'term', 'interestRate', 'installment', 'grade', 'subGrade', 'employmentTitle', 'employmentLength', 'homeOwnership', 'annualIncome', 'verificationStatus', 'purpose', 'postCode', 'regionCode', 'delinquency_2years', 'ficoRangeLow', 'ficoRangeHigh', 'openAcc', 'pubRec', 'pubRecBankruptcies', 'revolBal', 'totalAcc', 'initialListStatus', 'applicationType', 'title', 'n0', 'n1', 'n2', 'n3', 'n4', 'n5', 'n6', 'n7', 'n8', 'n9', 'n10', 'n11', 'n12', 'n13', 'n14', 'issueDate_month', 'issueDate_year', 'earliesCreditLine_month', 'earliesCreditLine_year', 'Credit_range_months']\n",
      "----------------------------------------------------------------------------\n",
      "Data size before divide the Nan Data: (1000000, 47)\n",
      "----------------------------------------------------------------------------\n",
      "Data size after divide the Nan Data: (999043, 47)\n",
      "The rows which have Nan Data: (957, 47)\n",
      "Full Feature rows shape: (999043, 45) (999043, 2)\n",
      "Number of Train_dataset (699330, 45) (699330, 2)\n",
      "Number of Test_dataset (299713, 45) (299713, 2)\n",
      "Random Forest Filling processing start...\n"
     ]
    }
   ],
   "source": [
    "Y_FeatureFillClassstr = 'dti/revolUtil'\n",
    "Y_FeatureFillClass=Y_FeatureFillClassstr.split('/')\n",
    "Full_feature =  All_data.describe().T['count'][All_data.describe().T['count'].values == All_data.shape[0]].index\n",
    "X_FeatureFill = [c for c in Full_feature if c not in (Y_FeatureFillClass+['id'])]\n",
    "print('----------------------------------------------------------------------------')\n",
    "print('Feature Filling Target:',len(Y_FeatureFillClass),'\\n',Y_FeatureFillClass)\n",
    "print('----------------------------------------------------------------------------')\n",
    "print('Feature Used for filling:',len(X_FeatureFill),'\\n',X_FeatureFill)\n",
    "print('----------------------------------------------------------------------------')\n",
    "All_data_Full = All_data[X_FeatureFill + Y_FeatureFillClass]\n",
    "\n",
    "print('Data size before divide the Nan Data:',All_data_Full.shape)\n",
    "print('----------------------------------------------------------------------------')\n",
    "\n",
    "All_data_Null = All_data_Full[All_data_Full.isnull().T.any()]\n",
    "All_data_Full = All_data_Full.dropna(axis = 0)\n",
    "\n",
    "print('Data size after divide the Nan Data:',All_data_Full.shape)\n",
    "print('The rows which have Nan Data:',All_data_Null.shape)\n",
    "X_FillTarget=All_data_Full[X_FeatureFill]\n",
    "y_FillTarget=All_data_Full[Y_FeatureFillClass]\n",
    "print('Full Feature rows shape:',X_FillTarget.shape,y_FillTarget.shape)\n",
    "X_Train, X_Test, y_Train, y_Test = train_test_split(X_FillTarget,y_FillTarget,\n",
    "                                                    test_size = 0.3,\n",
    "                                                    random_state = 233)\n",
    "print('Number of Train_dataset',X_Train.shape,y_Train.shape)\n",
    "print('Number of Test_dataset',X_Test.shape,y_Test.shape)\n",
    "print('Random Forest Filling processing start...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Target Feature now is: employmentTitle\n",
      "Wall time: 12.2 s\n",
      "Lgb acc score: 8532053.870\n",
      "----------------------------------------------------------------------------\n",
      "The Target Feature now is: postCode\n",
      "Wall time: 11.7 s\n",
      "Lgb acc score: 244.680\n",
      "----------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for var in Y_FeatureFillClass:\n",
    "    print('The Target Feature now is:',var)\n",
    "#------------------------Key code for Filling Model---------------------------#       \n",
    "    %time Lgb = RandomForestRegressor(n_estimators=15,random_state =9,n_jobs=-1).fit(X_Train,y_Train[var])\n",
    "    print(\"Lgb acc score: %.3f\" % accuracy_score(y_Test[var],Lgb.predict(X_Test)))\n",
    "    print('----------------------------------------------------------------------------')\n",
    "    Fill_Temp_values = Lgb.predict(All_data_Null[All_data_Null[var].isnull().values==True][X_FeatureFill])\n",
    "#------------------------Key code for Filling Model---------------------------#     \n",
    "    Fill_Temp_indexs = All_data_Null[All_data_Null[var].isnull().values==True].index\n",
    "    All_data_Null[var][Fill_Temp_indexs] = Fill_Temp_values \n",
    "    del Lgb,Fill_Temp_values,Fill_Temp_indexs\n",
    "    \n",
    "Fill_data = pd.concat([All_data_Null,All_data_Full],axis=0)\n",
    "All_data[Y_FeatureFillClass] = Fill_data[Y_FeatureFillClass]\n",
    "del Fill_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "All_data.to_csv(datapath+'All_data_NewStep3.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Feature Engineering 特征工程\n",
    "### 3.1第一轮特征工程（对数据进行分桶）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "All_data = pd.read_csv(datapath + 'All_data_NewStep3.csv',encoding='utf-8',delimiter=',')\n",
    "All_Y = pd.read_csv(datapath + 'Y_Final.csv',encoding='utf-8',delimiter=',')\n",
    "All_data['isDefault'] = All_Y\n",
    "Normal_Columns=set(All_data.columns)-set(['id','isDefault'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage of dataframe is 373.84 MB\n",
      "Memory usage after optimization is: 89.65 MB\n",
      "Decreased by 76.0%\n",
      "All_train size (800000, 47) All_test size (200000, 47)\n",
      "LGB AUC Train score: 0.7389  LGB AUC Test score: 0.7293\n"
     ]
    }
   ],
   "source": [
    "All_data = reduce_mem_usage(All_data)\n",
    "fea_importance = fast_test_lgbauc(All_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "All_data['employmentTitle_count'] = All_data.groupby(['employmentTitle'])['id'].transform('count')\n",
    "All_data['postCode_count'] = All_data.groupby(['postCode'])['id'].transform('count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "All_data['employmentTitle'][All_data['employmentTitle_count']<100] = np.nan\n",
    "All_data['postCode'][All_data['postCode_count']<100] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------\n",
      "Feature Filling Target: 2 \n",
      " ['employmentTitle', 'postCode']\n",
      "----------------------------------------------------------------------------\n",
      "Feature Used for filling: 47 \n",
      " ['loanAmnt', 'term', 'interestRate', 'installment', 'grade', 'subGrade', 'employmentLength', 'homeOwnership', 'annualIncome', 'verificationStatus', 'purpose', 'regionCode', 'dti', 'delinquency_2years', 'ficoRangeLow', 'ficoRangeHigh', 'openAcc', 'pubRec', 'pubRecBankruptcies', 'revolBal', 'revolUtil', 'totalAcc', 'initialListStatus', 'applicationType', 'title', 'n0', 'n1', 'n2', 'n3', 'n4', 'n5', 'n6', 'n7', 'n8', 'n9', 'n10', 'n11', 'n12', 'n13', 'n14', 'issueDate_month', 'issueDate_year', 'earliesCreditLine_month', 'earliesCreditLine_year', 'Credit_range_months', 'employmentTitle_count', 'postCode_count']\n",
      "----------------------------------------------------------------------------\n",
      "Data size before divide the Nan Data: (1000000, 49)\n",
      "----------------------------------------------------------------------------\n",
      "Data size after divide the Nan Data: (422972, 49)\n",
      "The rows which have Nan Data: (577028, 49)\n",
      "Full Feature rows shape: (422972, 47) (422972, 2)\n",
      "Number of Train_dataset (296080, 47) (296080, 2)\n",
      "Number of Test_dataset (126892, 47) (126892, 2)\n",
      "Random Forest Filling processing start...\n"
     ]
    }
   ],
   "source": [
    "Y_FeatureFillClassstr = 'employmentTitle/postCode'\n",
    "Y_FeatureFillClass=Y_FeatureFillClassstr.split('/')\n",
    "Full_feature =  All_data.describe().T['count'][All_data.describe().T['count'].values == All_data.shape[0]].index\n",
    "X_FeatureFill = [c for c in Full_feature if c not in (Y_FeatureFillClass+['id'])]\n",
    "print('----------------------------------------------------------------------------')\n",
    "print('Feature Filling Target:',len(Y_FeatureFillClass),'\\n',Y_FeatureFillClass)\n",
    "print('----------------------------------------------------------------------------')\n",
    "print('Feature Used for filling:',len(X_FeatureFill),'\\n',X_FeatureFill)\n",
    "print('----------------------------------------------------------------------------')\n",
    "All_data_Full = All_data[X_FeatureFill + Y_FeatureFillClass]\n",
    "\n",
    "print('Data size before divide the Nan Data:',All_data_Full.shape)\n",
    "print('----------------------------------------------------------------------------')\n",
    "\n",
    "All_data_Null = All_data_Full[All_data_Full.isnull().T.any()]\n",
    "All_data_Full = All_data_Full.dropna(axis = 0)\n",
    "\n",
    "print('Data size after divide the Nan Data:',All_data_Full.shape)\n",
    "print('The rows which have Nan Data:',All_data_Null.shape)\n",
    "X_FillTarget=All_data_Full[X_FeatureFill]\n",
    "y_FillTarget=All_data_Full[Y_FeatureFillClass]\n",
    "print('Full Feature rows shape:',X_FillTarget.shape,y_FillTarget.shape)\n",
    "X_Train, X_Test, y_Train, y_Test = train_test_split(X_FillTarget,y_FillTarget,\n",
    "                                                    test_size = 0.3,\n",
    "                                                    random_state = 233)\n",
    "print('Number of Train_dataset',X_Train.shape,y_Train.shape)\n",
    "print('Number of Test_dataset',X_Test.shape,y_Test.shape)\n",
    "print('Random Forest Filling processing start...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Target Feature now is: employmentTitle\n",
      "Wall time: 2min 7s\n",
      "Lgb acc score: 0.039\n",
      "----------------------------------------------------------------------------\n",
      "The Target Feature now is: postCode\n",
      "Wall time: 1min 21s\n",
      "Lgb acc score: 0.011\n",
      "----------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for var in Y_FeatureFillClass:\n",
    "    print('The Target Feature now is:',var)\n",
    "#------------------------Key code for Filling Model---------------------------#       \n",
    "    %time Lgb = LGBMClassifier(n_estimators=15,random_state =9,n_jobs=-1).fit(X_Train,y_Train[var])\n",
    "    print(\"Lgb acc score: %.3f\" % accuracy_score(y_Test[var],Lgb.predict(X_Test)))\n",
    "    print('----------------------------------------------------------------------------')\n",
    "    Fill_Temp_values = Lgb.predict(All_data_Null[All_data_Null[var].isnull().values==True][X_FeatureFill])\n",
    "#------------------------Key code for Filling Model---------------------------#     \n",
    "    Fill_Temp_indexs = All_data_Null[All_data_Null[var].isnull().values==True].index\n",
    "    All_data_Null[var][Fill_Temp_indexs] = Fill_Temp_values \n",
    "    del Lgb,Fill_Temp_values,Fill_Temp_indexs\n",
    "    \n",
    "Fill_data = pd.concat([All_data_Null,All_data_Full],axis=0)\n",
    "All_data[Y_FeatureFillClass] = Fill_data[Y_FeatureFillClass]\n",
    "del Fill_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "All_data['loanAmnt_bin2'] = np.floor_divide(All_data['loanAmnt'], 1000)\n",
    "All_data['interestRate_bin2'] = np.floor_divide(All_data['interestRate'],1)\n",
    "All_data['openAcc_bin2'] = np.floor_divide(All_data['openAcc'], 5)\n",
    "All_data['installment_bin2'] = np.floor_divide(All_data['installment'],50)\n",
    "All_data['annualIncome_bin3'] = np.floor(np.log1p(All_data['annualIncome']))\n",
    "All_data['regionCode_count'] = All_data.groupby(['regionCode'])['id'].transform('count')\n",
    "All_data['dti_bin3'] = np.floor(np.log1p(All_data['dti']))\n",
    "All_data['revolBal_bin3'] = np.floor(np.log1p(All_data['revolBal']))\n",
    "All_data['revolUtil_bin3'] = np.floor(np.log1p(All_data['revolUtil']))\n",
    "All_data['totalAcc_bin2'] = np.floor_divide(All_data['totalAcc'],5)\n",
    "All_data['title_count'] = All_data.groupby(['title'])['id'].transform('count')\n",
    "All_data['Credit_year'] = np.floor_divide(All_data['Credit_range_months'], 12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "All_data['loanAmnt_bin2'] = All_data['loanAmnt_bin2'].replace({0:1})\n",
    "All_data['term'] = All_data['term'].replace({3:0,5:1})\n",
    "All_data['interestRate_bin2'] = np.floor_divide(All_data['interestRate'],1)\n",
    "All_data['installment_bin2'].loc[All_data['installment_bin2']>25] = 25\n",
    "All_data['annualIncome_bin3'].loc[All_data['annualIncome_bin3']<8] = 8\n",
    "All_data['annualIncome_bin3'].loc[All_data['annualIncome_bin3']>14] = 14\n",
    "All_data['dti_bin3'] = All_data['dti_bin3'].replace({-inf:0,6:5})\n",
    "All_data['homeOwnership'] = All_data['homeOwnership'].replace({3:2,4:0,5:1})\n",
    "All_data['purpose'] = All_data['purpose'].replace({13:3})\n",
    "All_data['regionCode'] = All_data['regionCode'].replace({50:15})\n",
    "All_data['delinquency_2years'].loc[All_data['delinquency_2years']>10] = 10\n",
    "All_data['ficoRangeLow'].loc[All_data['ficoRangeLow']==630] = 660\n",
    "All_data['ficoRangeHigh'].loc[All_data['ficoRangeHigh']==634] = 664\n",
    "All_data['openAcc_bin2'].loc[All_data['openAcc_bin2']>10] = 10\n",
    "All_data['pubRec'].loc[All_data['pubRec']>10] = 10\n",
    "All_data['pubRecBankruptcies'].loc[All_data['pubRecBankruptcies']>5] = 5\n",
    "All_data['revolBal_bin3'] = All_data['revolBal_bin3'].replace({14:13})\n",
    "All_data['revolUtil_bin3'] = All_data['revolUtil_bin3'].replace({5:4,6:4})\n",
    "All_data['totalAcc_bin2'].loc[All_data['totalAcc_bin2']>20] = 20\n",
    "All_data['Credit_year'].loc[All_data['Credit_year']>50] = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage of dataframe is 171.66 MB\n",
      "Memory usage after optimization is: 122.07 MB\n",
      "Decreased by 28.9%\n",
      "All_train size (800000, 61) All_test size (200000, 61)\n",
      "LGB AUC Train score: 0.7396  LGB AUC Test score: 0.7299\n"
     ]
    }
   ],
   "source": [
    "All_data = reduce_mem_usage(All_data)\n",
    "fea_importance = fast_test_lgbauc(All_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "All_data.to_csv(datapath+'All_data_NewStep4.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 添加woe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "All_data = pd.read_csv(datapath + 'All_data_NewStep4.csv',encoding='utf-8',delimiter=',')\n",
    "All_Y = pd.read_csv(datapath + 'Y_Final.csv',encoding='utf-8',delimiter=',')\n",
    "All_data['isDefault'] = All_Y\n",
    "Data_columns_bef = list(All_data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binning_cate(df,col,target='isDefault'):   \n",
    "    \n",
    "    total = df[target].count()\n",
    "    bad = df[target].sum()\n",
    "    good = total-bad\n",
    "    \n",
    "    group = df.groupby([col],as_index=True)\n",
    "    bin_df = pd.DataFrame()\n",
    "\n",
    "    bin_df['total'] = group[target].count()\n",
    "    bin_df['totalrate'] = bin_df['total']/total\n",
    "    bin_df['bad'] = group[target].sum()\n",
    "    bin_df['badrate'] = bin_df['bad']/bin_df['total']\n",
    "    bin_df['good'] = bin_df['total'] - bin_df['bad']\n",
    "    bin_df['goodrate'] = bin_df['good']/bin_df['total']\n",
    "    bin_df['badattr'] = bin_df['bad']/bad\n",
    "    bin_df['goodattr'] = (bin_df['total']-bin_df['bad'])/good\n",
    "    bin_df['woe'] = np.log(bin_df['badattr']/bin_df['goodattr'])\n",
    "    bin_df['bin_iv'] = (bin_df['badattr']-bin_df['goodattr'])*bin_df['woe']\n",
    "    bin_df['iv'] = bin_df['bin_iv'].sum()\n",
    "    \n",
    "    return bin_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of feature in cate 38\n",
      "Select of feature in cate 14\n"
     ]
    }
   ],
   "source": [
    "Train_df =  All_data[:len(Train_data)]\n",
    "cate_col_str1 = 'loanAmnt_bin2/term/interestRate_bin2/installment_bin2/annualIncome_bin3/dti_bin3/homeOwnership'\n",
    "cate_col_str2 = 'purpose/regionCode/delinquency_2years/ficoRangeLow/ficoRangeHigh/openAcc_bin2/pubRec/pubRecBankruptcies'\n",
    "cate_col_str3 = 'revolBal_bin3/revolUtil_bin3/n1/n2/n3/n4/n5/n6/n7/n8/n9/n10/n11/n12/n13/n14'\n",
    "cate_col_str4 = 'issueDate_month/issueDate_year/Credit_year/regionCode_count/employmentTitle_count/title_count/installment_bin2'\n",
    "cate_col = cate_col_str1.split('/')+cate_col_str2.split('/')+cate_col_str3.split('/')+cate_col_str4.split('/')\n",
    "\n",
    "cate_bin_df_list=[]\n",
    "for col in cate_col:\n",
    "    bin_df = binning_cate(Train_df,col,'isDefault')\n",
    "    cate_bin_df_list.append(bin_df)\n",
    "    \n",
    "cate_iv_df = pd.DataFrame({'col':cate_col,'iv':[x['iv'].iloc[0] for x in cate_bin_df_list]}).sort_values('iv',ascending=False).reset_index(drop=True)\n",
    "print('Number of feature in cate',len(cate_iv_df))\n",
    "iv_select_cate_col = list(cate_iv_df[cate_iv_df.iv>0.03]['col'])\n",
    "print('Select of feature in cate',len(iv_select_cate_col))\n",
    "\n",
    "for col in iv_select_cate_col:\n",
    "    woe_dict = dict([x for x in cate_bin_df_list if x.index.name==col][0]['woe'])\n",
    "    All_data[col+'_woe'] = All_data[col].map(woe_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage of dataframe is 579.83 MB\n",
      "Memory usage after optimization is: 146.87 MB\n",
      "Decreased by 74.7%\n",
      "All_train size (800000, 74) All_test size (200000, 74)\n",
      "LGB AUC Train score: 0.7418  LGB AUC Test score: 0.7314\n",
      "Already drop the feature with no improvemnt,New Feature size: 69\n"
     ]
    }
   ],
   "source": [
    "All_data = reduce_mem_usage(All_data)\n",
    "Data_columns_aft = list(All_data.columns)\n",
    "fea_importance = fast_test_lgbauc(All_data)\n",
    "Rubbish_feature = list(fea_importance.loc[fea_importance['importance']==0]['feature_name'])\n",
    "New_fea = list(set(Data_columns_aft) - set(Data_columns_bef))\n",
    "New_fea_drop = [i for i in New_fea if i in Rubbish_feature]\n",
    "All_data = All_data.drop(New_fea_drop,axis=1)\n",
    "print('Already drop the feature with no improvemnt,New Feature size:',All_data.shape[1]-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "All_data['ficoRangeLow_woe'] = All_data['ficoRangeLow_woe'].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "All_data.to_csv(datapath+'All_data_NewStep5.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 添加地区（regionCode）交叉函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "All_data = pd.read_csv(datapath + 'All_data_NewStep5.csv',encoding='utf-8',delimiter=',')\n",
    "All_Y = pd.read_csv(datapath + 'Y_Final.csv',encoding='utf-8',delimiter=',')\n",
    "All_data['isDefault'] = All_Y\n",
    "Data_columns_bef = list(All_data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_cross(All_data,GroupFeature='regionCode',CorssFeature='loanAmnt'):\n",
    "    Train_df =  All_data[:len(Train_data)]\n",
    "    Train_gb = Train_df.groupby(GroupFeature)\n",
    "    all_info = {}\n",
    "    for kind, kind_data in Train_gb:\n",
    "    \n",
    "        info = {}\n",
    "        kind_data = kind_data[kind_data[CorssFeature] > 0]\n",
    "        info[GroupFeature+'_'+CorssFeature+'_max'] = kind_data[CorssFeature].max()\n",
    "        info[GroupFeature+'_'+CorssFeature+'_median'] = kind_data[CorssFeature].median()\n",
    "        info[GroupFeature+'_'+CorssFeature+'_min'] = kind_data[CorssFeature].min()\n",
    "        info[GroupFeature+'_'+CorssFeature+'_sum'] = kind_data[CorssFeature].sum()\n",
    "        info[GroupFeature+'_'+CorssFeature+'_std'] = kind_data[CorssFeature].std()\n",
    "        info[GroupFeature+'_'+CorssFeature+'_mean'] = kind_data[CorssFeature].mean()\n",
    "        info[GroupFeature+'_'+CorssFeature+'_skew'] = kind_data[CorssFeature].skew()\n",
    "        info[GroupFeature+'_'+CorssFeature+'_kurt'] = kind_data[CorssFeature].kurt()\n",
    "        info[GroupFeature+'_'+CorssFeature+'_mad'] = kind_data[CorssFeature].mad()\n",
    "\n",
    "        all_info[kind] = info\n",
    "        \n",
    "    brand_fe = pd.DataFrame(all_info).T.reset_index().rename(columns={\"index\": GroupFeature})\n",
    "    All_data = All_data.merge(brand_fe, how='left', on=GroupFeature)\n",
    "    \n",
    "    return All_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 5.5 s\n",
      "Wall time: 2.1 s\n",
      "Wall time: 2.27 s\n",
      "Wall time: 2.5 s\n"
     ]
    }
   ],
   "source": [
    "%time All_data = feature_cross(All_data,GroupFeature='regionCode',CorssFeature='loanAmnt')\n",
    "%time All_data = feature_cross(All_data,GroupFeature='regionCode',CorssFeature='annualIncome')\n",
    "%time All_data = feature_cross(All_data,GroupFeature='regionCode',CorssFeature='dti')\n",
    "%time All_data = feature_cross(All_data,GroupFeature='regionCode',CorssFeature='revolBal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage of dataframe is 823.97 MB\n",
      "Memory usage after optimization is: 230.79 MB\n",
      "Decreased by 72.0%\n",
      "All_train size (800000, 105) All_test size (200000, 105)\n",
      "LGB AUC Train score: 0.7424  LGB AUC Test score: 0.7323\n",
      "Already drop the feature with no improvemnt,New Feature size: 104\n"
     ]
    }
   ],
   "source": [
    "All_data = reduce_mem_usage(All_data)\n",
    "Data_columns_aft = list(All_data.columns)\n",
    "fea_importance = fast_test_lgbauc(All_data)\n",
    "Rubbish_feature = list(fea_importance.loc[fea_importance['importance']==0]['feature_name'])\n",
    "New_fea = list(set(Data_columns_aft) - set(Data_columns_bef))\n",
    "New_fea_drop = [i for i in New_fea if i in Rubbish_feature]\n",
    "All_data = All_data.drop(New_fea_drop,axis=1)\n",
    "print('Already drop the feature with no improvemnt,New Feature size:',All_data.shape[1]-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "All_data.to_csv(datapath+'All_data_NewStep6.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 就业职称（employmentTitle）交叉函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "All_data = pd.read_csv(datapath + 'All_data_NewStep6.csv',encoding='utf-8',delimiter=',')\n",
    "All_Y = pd.read_csv(datapath + 'Y_Final.csv',encoding='utf-8',delimiter=',')\n",
    "All_data['isDefault'] = All_Y\n",
    "Data_columns_bef = list(All_data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 9.17 s\n",
      "Wall time: 4.19 s\n",
      "Wall time: 4.44 s\n",
      "Wall time: 4.54 s\n"
     ]
    }
   ],
   "source": [
    "%time All_data = feature_cross(All_data,GroupFeature='employmentTitle',CorssFeature='loanAmnt')\n",
    "%time All_data = feature_cross(All_data,GroupFeature='employmentTitle',CorssFeature='annualIncome')\n",
    "%time All_data = feature_cross(All_data,GroupFeature='employmentTitle',CorssFeature='dti')\n",
    "%time All_data = feature_cross(All_data,GroupFeature='employmentTitle',CorssFeature='revolBal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage of dataframe is 1091.00 MB\n",
      "Memory usage after optimization is: 322.34 MB\n",
      "Decreased by 70.5%\n",
      "All_train size (800000, 140) All_test size (200000, 140)\n",
      "LGB AUC Train score: 0.7426  LGB AUC Test score: 0.7322\n",
      "Already drop the feature with no improvemnt,New Feature size: 137\n"
     ]
    }
   ],
   "source": [
    "All_data = reduce_mem_usage(All_data)\n",
    "Data_columns_aft = list(All_data.columns)\n",
    "fea_importance = fast_test_lgbauc(All_data)\n",
    "Rubbish_feature = list(fea_importance.loc[fea_importance['importance']==0]['feature_name'])\n",
    "New_fea = list(set(Data_columns_aft) - set(Data_columns_bef))\n",
    "New_fea_drop = [i for i in New_fea if i in Rubbish_feature]\n",
    "All_data = All_data.drop(New_fea_drop,axis=1)\n",
    "print('Already drop the feature with no improvemnt,New Feature size:',All_data.shape[1]-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "All_data.to_csv(datapath+'All_data_NewStep7.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 邮政编码的前3位数字（postCode）交叉函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "All_data = pd.read_csv(datapath + 'All_data_NewStep7.csv',encoding='utf-8',delimiter=',')\n",
    "All_Y = pd.read_csv(datapath + 'Y_Final.csv',encoding='utf-8',delimiter=',')\n",
    "All_data['isDefault'] = All_Y\n",
    "Data_columns_bef = list(All_data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 12 s\n",
      "Wall time: 5.38 s\n",
      "Wall time: 5.51 s\n",
      "Wall time: 5.67 s\n"
     ]
    }
   ],
   "source": [
    "%time All_data = feature_cross(All_data,GroupFeature='postCode',CorssFeature='loanAmnt')\n",
    "%time All_data = feature_cross(All_data,GroupFeature='postCode',CorssFeature='annualIncome')\n",
    "%time All_data = feature_cross(All_data,GroupFeature='postCode',CorssFeature='dti')\n",
    "%time All_data = feature_cross(All_data,GroupFeature='postCode',CorssFeature='revolBal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage of dataframe is 1342.77 MB\n",
      "Memory usage after optimization is: 406.27 MB\n",
      "Decreased by 69.7%\n",
      "All_train size (800000, 173) All_test size (200000, 173)\n",
      "LGB AUC Train score: 0.7430  LGB AUC Test score: 0.7320\n",
      "Already drop the feature with no improvemnt,New Feature size: 172\n"
     ]
    }
   ],
   "source": [
    "All_data = reduce_mem_usage(All_data)\n",
    "Data_columns_aft = list(All_data.columns)\n",
    "fea_importance = fast_test_lgbauc(All_data)\n",
    "Rubbish_feature = list(fea_importance.loc[fea_importance['importance']==0]['feature_name'])\n",
    "New_fea = list(set(Data_columns_aft) - set(Data_columns_bef))\n",
    "New_fea_drop = [i for i in New_fea if i in Rubbish_feature]\n",
    "All_data = All_data.drop(New_fea_drop,axis=1)\n",
    "print('Already drop the feature with no improvemnt,New Feature size:',All_data.shape[1]-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "All_data.to_csv(datapath+'All_data_NewStep8.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6 其他交叉函数（及补充）\n",
    "homeOwnership-交叉无效<br>\n",
    "verificationStatus-负向优化<br>\n",
    "employmentLength-负向优化<br>\n",
    "purpose-负向优化<br>\n",
    "ficoRangeLow-负向优化<br>\n",
    "openAcc-负向优化<br>\n",
    "pubRec-负向优化<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "All_data = pd.read_csv(datapath + 'All_data_NewStep8.csv',encoding='utf-8',delimiter=',')\n",
    "All_Y = pd.read_csv(datapath + 'Y_Final.csv',encoding='utf-8',delimiter=',')\n",
    "All_data['isDefault'] = All_Y\n",
    "Data_columns_bef = list(All_data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 12.3 s\n",
      "Wall time: 5.48 s\n",
      "Wall time: 6.42 s\n"
     ]
    }
   ],
   "source": [
    "%time All_data = feature_cross(All_data,GroupFeature='regionCode',CorssFeature='interestRate')\n",
    "%time All_data = feature_cross(All_data,GroupFeature='employmentTitle',CorssFeature='interestRate')\n",
    "%time All_data = feature_cross(All_data,GroupFeature='postCode',CorssFeature='interestRate')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage of dataframe is 1541.14 MB\n",
      "Memory usage after optimization is: 461.58 MB\n",
      "Decreased by 70.0%\n",
      "All_train size (800000, 199) All_test size (200000, 199)\n",
      "LGB AUC Train score: 0.7432  LGB AUC Test score: 0.7321\n",
      "Already drop the feature with no improvemnt,New Feature size: 192\n"
     ]
    }
   ],
   "source": [
    "All_data = reduce_mem_usage(All_data)\n",
    "Data_columns_aft = list(All_data.columns)\n",
    "fea_importance = fast_test_lgbauc(All_data)\n",
    "Rubbish_feature = list(fea_importance.loc[fea_importance['importance']==0]['feature_name'])\n",
    "New_fea = list(set(Data_columns_aft) - set(Data_columns_bef))\n",
    "New_fea_drop = [i for i in New_fea if i in Rubbish_feature]\n",
    "All_data = All_data.drop(New_fea_drop,axis=1)\n",
    "print('Already drop the feature with no improvemnt,New Feature size:',All_data.shape[1]-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "All_data.to_csv(datapath+'All_data_NewStep9.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.7 少类型种类进行One_hot编码(反向优化最为致命)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "~~~\n",
    "All_data = pd.read_csv(datapath + 'All_data_NewStep9.csv',encoding='utf-8',delimiter=',')\n",
    "All_Y = pd.read_csv(datapath + 'Y_Final.csv',encoding='utf-8',delimiter=',')\n",
    "All_data['isDefault'] = All_Y\n",
    "Data_columns_bef = list(All_data.columns)\n",
    "~~~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "~~~\n",
    "All_data['ficoRangeMinus']=All_data['ficoRangeHigh']-All_data['ficoRangeLow']\n",
    "\n",
    "onehot_columnstr='term/grade/homeOwnership/verificationStatus/ficoRangeMinus/initialListStatus/applicationType'\n",
    "onehot_column = onehot_columnstr.split('/')\n",
    "All_data = pd.get_dummies(All_data,columns=onehot_column,drop_first=True)\n",
    "~~~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "~~~\n",
    "All_data = reduce_mem_usage(All_data)\n",
    "Data_columns_aft = list(All_data.columns)\n",
    "fea_importance = fast_test_lgbauc(All_data)\n",
    "Rubbish_feature = list(fea_importance.loc[fea_importance['importance']==0]['feature_name'])\n",
    "New_fea = list(set(Data_columns_aft) - set(Data_columns_bef))\n",
    "New_fea_drop = [i for i in New_fea if i in Rubbish_feature]\n",
    "All_data = All_data.drop(New_fea_drop,axis=1)\n",
    "print('Already drop the feature with no improvemnt,New Feature size:',All_data.shape[1]-2)\n",
    "\n",
    "~~~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.8 特征选择(删除重要性小于5的垃圾特征)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage of dataframe is 1480.10 MB\n",
      "Memory usage after optimization is: 436.78 MB\n",
      "Decreased by 70.5%\n"
     ]
    }
   ],
   "source": [
    "All_data = pd.read_csv(datapath + 'All_data_NewStep9.csv',encoding='utf-8',delimiter=',')\n",
    "All_Y = pd.read_csv(datapath + 'Y_Final.csv',encoding='utf-8',delimiter=',')\n",
    "#All_data = All_data.drop(['id','isDefault'],axis=1)\n",
    "All_data = reduce_mem_usage(All_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All_train size (800000, 192) All_test size (200000, 192)\n",
      "LGB AUC Train score: 0.7432  LGB AUC Test score: 0.7321\n",
      "Already drop the feature with no improvemnt,New Feature size: 104\n",
      "All_train size (800000, 104) All_test size (200000, 104)\n",
      "LGB AUC Train score: 0.7431  LGB AUC Test score: 0.7325\n"
     ]
    }
   ],
   "source": [
    "Data_columns_all = list(All_data.columns)\n",
    "fea_importance = fast_test_lgbauc(All_data)\n",
    "Rubbish_feature = list(fea_importance.loc[fea_importance['importance']<5]['feature_name'])\n",
    "New_fea = list(set(Data_columns_all) - set(Rubbish_feature))\n",
    "All_data = All_data.drop(Rubbish_feature,axis=1)\n",
    "print('Already drop the feature with no improvemnt,New Feature size:',All_data.shape[1]-2)\n",
    "fea_importance = fast_test_lgbauc(All_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "All_data.to_csv(datapath+'All_data_NewStep10.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.模型训练及参数查找(lgb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 LGB最佳参数测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage of dataframe is 793.46 MB\n",
      "Memory usage after optimization is: 217.44 MB\n",
      "Decreased by 72.6%\n",
      "All_train size (800000, 104) All_test size (200000, 104)\n",
      "--------------------------TrainDataset--------------------------\n",
      "xtrain size is : (800000, 104) ytrain size is : (800000, 1)\n",
      "--------------------------TrainDataset--------------------------\n",
      "xtest size is : (200000, 104)\n"
     ]
    }
   ],
   "source": [
    "All_data = pd.read_csv(datapath + 'All_data_NewStep10.csv',encoding='utf-8',delimiter=',')\n",
    "All_Y = pd.read_csv(datapath + 'Y_Final.csv',encoding='utf-8',delimiter=',')\n",
    "All_data = All_data.drop(['id','isDefault'],axis=1)\n",
    "All_data = reduce_mem_usage(All_data)\n",
    "\n",
    "n_train =len(All_Y)\n",
    "\n",
    "All_train =  All_data[:n_train] \n",
    "All_test = All_data[n_train:] \n",
    "print('All_train size',All_train.shape,'All_test size',All_test.shape)\n",
    "del All_data\n",
    "\n",
    "X_Train = All_train\n",
    "y_Train = All_Y\n",
    "X_Test = All_test\n",
    "print('--------------------------TrainDataset--------------------------')\n",
    "print('xtrain size is : {}'.format(X_Train.shape),'ytrain size is : {}'.format(y_Train.shape))\n",
    "print('--------------------------TrainDataset--------------------------')\n",
    "print('xtest size is : {}'.format(X_Test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "params_dic = {    \n",
    "          'boosting_type': 'gbdt',\n",
    "          'objective': 'binary',\n",
    "          'metric': 'auc',\n",
    "          'nthread':4,\n",
    "          'learning_rate':0.1,\n",
    "          'num_leaves':30, \n",
    "          'max_depth': 5,   \n",
    "          'subsample': 0.8, \n",
    "          'colsample_bytree': 0.8, \n",
    "    }\n",
    "\n",
    "data_train = lgb.Dataset(X_Train, y_Train)\n",
    "cv_results = lgb.cv(params=params_dic,train_set = data_train, num_boost_round=1000, nfold=5, stratified=False, \n",
    "                    shuffle=True, metrics='auc',early_stopping_rounds=50,\n",
    "                    verbose_eval = 0,seed=0)\n",
    "print('best n_estimators:', len(cv_results['auc-mean']))\n",
    "print('best cv score:', pd.Series(cv_results['auc-mean']).max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------Finding Parameters, params_test1 --------------------------\n",
      "Parameters: params_test1 {'max_depth': range(3, 8), 'num_leaves': range(5, 100, 5)}\n",
      "Start Fiting, params_test1\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-175-fa9957206e4a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Start Fiting,'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mparams_string\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 38\u001b[1;33m     \u001b[0mget_ipython\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmagic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'time gsearch.fit(X_Train,y_Train)'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     39\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgsearch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbest_params_\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'Best evaluation Scores'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mgsearch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbest_score_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m     \u001b[0minitial_params\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgsearch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbest_params_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Administrator\\.conda\\envs\\py37\\lib\\site-packages\\IPython\\core\\interactiveshell.py\u001b[0m in \u001b[0;36mmagic\u001b[1;34m(self, arg_s)\u001b[0m\n\u001b[0;32m   2158\u001b[0m         \u001b[0mmagic_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmagic_arg_s\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marg_s\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpartition\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m' '\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2159\u001b[0m         \u001b[0mmagic_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmagic_name\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlstrip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprefilter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mESC_MAGIC\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2160\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmagic_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmagic_arg_s\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2161\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2162\u001b[0m     \u001b[1;31m#-------------------------------------------------------------------------\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Administrator\\.conda\\envs\\py37\\lib\\site-packages\\IPython\\core\\interactiveshell.py\u001b[0m in \u001b[0;36mrun_line_magic\u001b[1;34m(self, magic_name, line)\u001b[0m\n\u001b[0;32m   2079\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'local_ns'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getframe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstack_depth\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf_locals\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2080\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2081\u001b[1;33m                 \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2082\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2083\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<decorator-gen-54>\u001b[0m in \u001b[0;36mtime\u001b[1;34m(self, line, cell, local_ns)\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Administrator\\.conda\\envs\\py37\\lib\\site-packages\\IPython\\core\\magic.py\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(f, *a, **k)\u001b[0m\n\u001b[0;32m    186\u001b[0m     \u001b[1;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    187\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 188\u001b[1;33m         \u001b[0mcall\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    189\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    190\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Administrator\\.conda\\envs\\py37\\lib\\site-packages\\IPython\\core\\magics\\execution.py\u001b[0m in \u001b[0;36mtime\u001b[1;34m(self, line, cell, local_ns)\u001b[0m\n\u001b[0;32m   1187\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;34m'eval'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1188\u001b[0m             \u001b[0mst\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1189\u001b[1;33m             \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0meval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mglob\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlocal_ns\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1190\u001b[0m             \u001b[0mend\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1191\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<timed eval>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Administrator\\.conda\\envs\\py37\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     70\u001b[0m                           FutureWarning)\n\u001b[0;32m     71\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 72\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     73\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Administrator\\.conda\\envs\\py37\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[0;32m    734\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    735\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 736\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    737\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    738\u001b[0m         \u001b[1;31m# For multi-metric evaluation, store the best_index_, best_params_ and\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Administrator\\.conda\\envs\\py37\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[1;34m(self, evaluate_candidates)\u001b[0m\n\u001b[0;32m   1186\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1187\u001b[0m         \u001b[1;34m\"\"\"Search all candidates in param_grid\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1188\u001b[1;33m         \u001b[0mevaluate_candidates\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mParameterGrid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1189\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Administrator\\.conda\\envs\\py37\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[1;34m(candidate_params)\u001b[0m\n\u001b[0;32m    713\u001b[0m                                \u001b[1;32mfor\u001b[0m \u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    714\u001b[0m                                in product(candidate_params,\n\u001b[1;32m--> 715\u001b[1;33m                                           cv.split(X, y, groups)))\n\u001b[0m\u001b[0;32m    716\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    717\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Administrator\\.conda\\envs\\py37\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1059\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1060\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1061\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1062\u001b[0m             \u001b[1;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1063\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Administrator\\.conda\\envs\\py37\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    938\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    939\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'supports_timeout'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 940\u001b[1;33m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    941\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    942\u001b[0m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Administrator\\.conda\\envs\\py37\\lib\\site-packages\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36mwrap_future_result\u001b[1;34m(future, timeout)\u001b[0m\n\u001b[0;32m    540\u001b[0m         AsyncResults.get from multiprocessing.\"\"\"\n\u001b[0;32m    541\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 542\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfuture\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    543\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mCfTimeoutError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    544\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Administrator\\.conda\\envs\\py37\\lib\\concurrent\\futures\\_base.py\u001b[0m in \u001b[0;36mresult\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    428\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    429\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 430\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_condition\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    431\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    432\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mCANCELLED\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mCANCELLED_AND_NOTIFIED\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Administrator\\.conda\\envs\\py37\\lib\\threading.py\u001b[0m in \u001b[0;36mwait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    294\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m    \u001b[1;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    295\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 296\u001b[1;33m                 \u001b[0mwaiter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    297\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    298\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#由于搜索太慢采用贪心策略进行搜索（下为初始参数）\n",
    "initial_params ={'boosting_type':'gbdt',\n",
    "                 'objective':'binary',\n",
    "                 'metrics':'auc',\n",
    "                 'learning_rate':0.1,\n",
    "                 'n_estimators':575,\n",
    "                 'verbose':-1}\n",
    "\n",
    "\n",
    "# Single Wall time: 4min 27s\n",
    "#下面为需要调整的参数\n",
    "params_test1={'max_depth': range(3,8,1), 'num_leaves':range(5,100,5)}\n",
    "params_test2={'max_bin': range(5,256,10), 'min_data_in_leaf':range(1,102,10)}\n",
    "\n",
    "params_test3={'feature_fraction': [0.6,0.7,0.8,0.9,1.0],\n",
    "              'bagging_fraction': [0.6,0.7,0.8,0.9,1.0],\n",
    "              'bagging_freq': range(0,81,10)}\n",
    "\n",
    "params_test4={'lambda_l1': [1e-5,1e-3,1e-1,0.0,0.1,0.3,0.5,0.7,0.9,1.0],\n",
    "              'lambda_l2': [1e-5,1e-3,1e-1,0.0,0.1,0.3,0.5,0.7,0.9,1.0]}\n",
    "\n",
    "params_test5={'min_split_gain':[0.0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1.0]}\n",
    "\n",
    "params_turns = 4\n",
    "\n",
    "#开始采用贪心算法对参数进行搜索，返回的initial_params为获取的最佳参数\n",
    "df_cv_result=[]\n",
    "for i in range(params_turns):\n",
    "    params_string = \"params_test\"+str(i+1)\n",
    "    params_round = locals()[params_string]\n",
    "    print('----------------------Finding Parameters,',params_string,'--------------------------') \n",
    "    print('Parameters:',params_string,params_round)\n",
    "    \n",
    "    gsearch = GridSearchCV(estimator = LGBMClassifier(**initial_params), \n",
    "                       param_grid = params_round, scoring='roc_auc',cv=5,n_jobs=-1)\n",
    "    \n",
    "    print('Start Fiting,',params_string)          \n",
    "    %time gsearch.fit(X_Train,y_Train)\n",
    "    print(gsearch.best_params_,'Best evaluation Scores',gsearch.best_score_)\n",
    "    initial_params.update(gsearch.best_params_)\n",
    "    \n",
    "    df_cv_result.append(pd.DataFrame(gsearch.cv_results_))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 最佳LGB参数(未加n_estimators)\n",
    "```\n",
    "{'boosting_type': 'gbdt',\n",
    " 'objective': 'binary',\n",
    " 'metrics': 'auc',\n",
    " 'learning_rate': 0.1,\n",
    " 'n_estimators': 575,\n",
    " 'max_depth': 7,\n",
    " 'num_leaves': 80,\n",
    " 'verbose': -1,\n",
    " 'max_bin': 155,\n",
    " 'min_data_in_leaf': 101,\n",
    " 'bagging_fraction': 0.6,\n",
    " 'bagging_freq': 0,\n",
    " 'feature_fraction': 1.0,\n",
    " 'lambda_l1': 0.5,\n",
    " 'lambda_l2': 0.1,\n",
    " 'min_split_gain': 0.0}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 CatBoost调试最佳参数 (谷歌云上运行)\n",
    "__CatB AUC Train score: 0.7834 CatB AUC Test score: 0.7363__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "#由于搜索太慢采用贪心策略进行搜索（下为初始参数）\n",
    "\n",
    "initial_params ={'eval_metric':'AUC',\n",
    "                 'random_seed':42,                \n",
    "                 'verbose':0}\n",
    "\n",
    "#下面为需要调整的参数\n",
    "\n",
    "params_test1={'n_estimators':[10,20,50,100,200,500,1000]}\n",
    "params_test2={'depth': range(3,8,1)}\n",
    "params_test3={'learning_rate':[0.001,0.01,0.1,0.5,0.7,0.8,0.9,1.0]}\n",
    "params_test4={'l2_leaf_reg':[0.01,0.1,1,10,100]}\n",
    "params_test5={'border_count':[128,254]}\n",
    "\n",
    "\n",
    "params_turns = 5\n",
    "\n",
    "#开始采用贪心算法对参数进行搜索，返回的initial_params为获取的最佳参数\n",
    "df_cv_result=[]\n",
    "for i in range(params_turns):\n",
    "    params_string = \"params_test\"+str(i+1)\n",
    "    params_round = locals()[params_string]\n",
    "    print('----------------------Finding Parameters,',params_string,'--------------------------') \n",
    "    print('Parameters:',params_string,params_round)\n",
    "    \n",
    "    gsearch = GridSearchCV(estimator = CatBoostClassifier(**initial_params), \n",
    "                       param_grid = params_round, scoring='roc_auc',cv=5,n_jobs=-1)\n",
    "    \n",
    "    print('Start Fiting,',params_string)          \n",
    "    %time gsearch.fit(X_Train,y_Train)\n",
    "    print(gsearch.best_params_,'Best evaluation Scores',gsearch.best_score_)\n",
    "    initial_params.update(gsearch.best_params_)\n",
    "    \n",
    "    df_cv_result.append(pd.DataFrame(gsearch.cv_results_))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "此处代码为谷歌云上运行，云上结果如下\n",
    "```\n",
    "----------------------Finding Parameters, params_test1 --------------------------\n",
    "Parameters: params_test1 {'n_estimators': [10, 20, 50, 100, 200, 500, 1000]}\n",
    "Start Fiting, params_test1\n",
    "CPU times: user 21min 13s, sys: 23 s, total: 21min 36s\n",
    "Wall time: 13min 28s\n",
    "{'n_estimators': 1000} Best evaluation Scores 0.7370597109406334\n",
    "----------------------Finding Parameters, params_test2 --------------------------\n",
    "Parameters: params_test2 {'depth': range(3, 8)}\n",
    "Start Fiting, params_test2\n",
    "CPU times: user 15min 58s, sys: 17 s, total: 16min 15s\n",
    "Wall time: 26min 33s\n",
    "{'depth': 4} Best evaluation Scores 0.7379032052238406\n",
    "----------------------Finding Parameters, params_test3 --------------------------\n",
    "Parameters: params_test3 {'learning_rate': [0.001, 0.01, 0.1, 0.5, 0.7, 0.8, 0.9, 1.0]}\n",
    "Start Fiting, params_test3\n",
    "CPU times: user 16min 8s, sys: 16.5 s, total: 16min 25s\n",
    "Wall time: 37min 8s\n",
    "{'learning_rate': 0.1} Best evaluation Scores 0.7378817116059709\n",
    "----------------------Finding Parameters, params_test4 --------------------------\n",
    "Parameters: params_test4 {'l2_leaf_reg': [0.01, 0.1, 1, 10, 100]}\n",
    "Start Fiting, params_test4\n",
    "CPU times: user 16min 34s, sys: 18.9 s, total: 16min 53s\n",
    "Wall time: 23min 7s\n",
    "{'l2_leaf_reg': 100} Best evaluation Scores 0.7379399636868315\n",
    "----------------------Finding Parameters, params_test5 --------------------------\n",
    "Parameters: params_test5 {'border_count': [128, 254]}\n",
    "Start Fiting, params_test5\n",
    "CPU times: user 16min 25s, sys: 17.8 s, total: 16min 43s\n",
    "Wall time: 10min 17s\n",
    "{'border_count': 128} Best evaluation Scores 0.7380043408466763\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 获取的最佳cat参数为\n",
    "```\n",
    "{'loss_function': 'Logloss',\n",
    " 'learning_rate': 0.1,\n",
    " 'random_seed': 42,\n",
    " 'n_estimators': 931,\n",
    " 'depth': 6,\n",
    " 'verbose': 0,\n",
    " 'bootstrap_type': 'MVS',\n",
    " 'border_count': 145,\n",
    " 'min_data_in_leaf': 1,\n",
    " 'l2_leaf_reg': 100}\n",
    " ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3最佳LogisticRegression参数查找"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------Finding Parameters, params_test1 --------------------------\n",
      "Parameters: params_test1 {'C': [1e-05, 0.001, 0.1, 0.0, 0.1, 0.3, 0.5, 0.7, 0.9, 1.0, 10, 100, 1000]}\n",
      "Start Fiting, params_test1\n",
      "Wall time: 23min 29s\n",
      "{'C': 100} Best evaluation Scores 0.6001374029563713\n"
     ]
    }
   ],
   "source": [
    "#由于搜索太慢采用贪心策略进行搜索（下为初始参数）\n",
    "initial_params ={\n",
    "    \n",
    "}\n",
    "\n",
    "\n",
    "# Single Wall time: 4min 27s\n",
    "#下面为需要调整的参数\n",
    "params_test1={'C': [1e-5,1e-3,1e-1,0.0,0.1,0.3,0.5,0.7,0.9,1.0,10,100,1000]}\n",
    "\n",
    "\n",
    "params_turns = 1\n",
    "\n",
    "#开始采用贪心算法对参数进行搜索，返回的initial_params为获取的最佳参数\n",
    "df_cv_result=[]\n",
    "for i in range(params_turns):\n",
    "    params_string = \"params_test\"+str(i+1)\n",
    "    params_round = locals()[params_string]\n",
    "    print('----------------------Finding Parameters,',params_string,'--------------------------') \n",
    "    print('Parameters:',params_string,params_round)\n",
    "    \n",
    "    gsearch = GridSearchCV(estimator = LogisticRegression(**initial_params), \n",
    "                       param_grid = params_round, scoring='roc_auc',cv=5,n_jobs=-1)\n",
    "    \n",
    "    print('Start Fiting,',params_string)          \n",
    "    %time gsearch.fit(X_Train,y_Train)\n",
    "    print(gsearch.best_params_,'Best evaluation Scores',gsearch.best_score_)\n",
    "    initial_params.update(gsearch.best_params_)\n",
    "    \n",
    "    df_cv_result.append(pd.DataFrame(gsearch.cv_results_))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb_params = {\n",
    "'boosting_type': 'gbdt',\n",
    "'objective': 'binary',\n",
    "'metrics': 'auc',\n",
    "'learning_rate': 0.1,\n",
    "'n_estimators': 631,\n",
    "'max_depth': 5,\n",
    "'num_leaves': 35,\n",
    "'verbose': -1,\n",
    "'max_bin': 155,\n",
    "'min_data_in_leaf': 101,\n",
    "'bagging_fraction': 0.6,\n",
    "'bagging_freq': 0,\n",
    "'feature_fraction': 1.0,\n",
    "'lambda_l1': 0.5,\n",
    "'lambda_l2': 0.1,\n",
    "'min_split_gain': 0.0,\n",
    "'n_jobs':-1\n",
    "             }\n",
    "\n",
    "CatB_params = {\n",
    "'loss_function': 'Logloss',\n",
    "'learning_rate': 0.1,\n",
    "'n_estimators': 931,\n",
    "'depth': 6,\n",
    "'verbose': 0,\n",
    "'bootstrap_type': 'MVS',\n",
    "'border_count': 145,\n",
    "'min_data_in_leaf': 1,\n",
    "'l2_leaf_reg': 100\n",
    "}\n",
    "\n",
    "rf_params = {\n",
    "'n_estimators':300,                   \n",
    "'criterion':'gini',                   \n",
    "'max_depth':15,                   \n",
    "'min_samples_split':50,                   \n",
    "'min_samples_leaf':20,                   \n",
    "'max_features':0.5,                   \n",
    "'max_leaf_nodes':None,                   \n",
    "'min_impurity_decrease':0.0,                   \n",
    "'n_jobs':-1}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Stacking Model\n",
    "### 5.1 定义一些Stacking 模型的基础函数和类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class to extend the Sklearn classifier\n",
    "class SklearnHelper(object):\n",
    "    def __init__(self, clf, seed=SEED, params=None):\n",
    "        params['random_state'] = seed\n",
    "        self.clf = clf(**params)\n",
    "\n",
    "    def train(self, x_train, y_train):\n",
    "        self.clf.fit(x_train, y_train)\n",
    "\n",
    "    def predict(self, x):\n",
    "        return self.clf.predict(x)\n",
    "    \n",
    "    def predict_proba(self, x):\n",
    "        return self.clf.predict_proba(x)\n",
    "    \n",
    "    def fit(self,x,y):\n",
    "        return self.clf.fit(x,y)\n",
    "    \n",
    "    def feature_importances(self,x,y):\n",
    "        print(self.clf.fit(x,y).feature_importances_)\n",
    "        \n",
    "def get_oof(clf, X_Train, y_Train, X_Test):\n",
    "    oof_train = np.zeros((n_train,))\n",
    "    oof_test = np.zeros((n_test,))\n",
    "    oof_test_skf = np.empty((NFOLDS, n_test))\n",
    "    X_Train.index = range(len(X_Train))\n",
    "    y_Train.index = range(len(y_Train))\n",
    "    for i, (train_index, test_index) in enumerate(kf.split(X_Train)):\n",
    "        X_tr = X_Train.loc[train_index]\n",
    "        y_tr = y_Train.loc[train_index]\n",
    "        X_te = X_Train.loc[test_index]\n",
    "\n",
    "        clf.train(X_tr, y_tr)\n",
    "\n",
    "        oof_train[test_index] = clf.predict_proba(X_te)[:,1]\n",
    "        oof_test_skf[i, :] = clf.predict_proba(X_Test)[:,1]\n",
    "\n",
    "    oof_test[:] = oof_test_skf.mean(axis=0)\n",
    "    return oof_train.reshape(-1, 1), oof_test.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage of dataframe is 793.46 MB\n",
      "Memory usage after optimization is: 217.44 MB\n",
      "Decreased by 72.6%\n",
      "All_train size (800000, 104) All_test size (200000, 104)\n",
      "--------------------------TrainDataset--------------------------\n",
      "xtrain size is : (800000, 104) ytrain size is : (800000, 1)\n",
      "--------------------------TrainDataset--------------------------\n",
      "xtest size is : (200000, 104)\n"
     ]
    }
   ],
   "source": [
    "All_data = pd.read_csv(datapath + 'All_data_NewStep10.csv',encoding='utf-8',delimiter=',')\n",
    "All_Y = pd.read_csv(datapath + 'Y_Final.csv',encoding='utf-8',delimiter=',')\n",
    "All_data = All_data.drop(['id','isDefault'],axis=1)\n",
    "All_data = reduce_mem_usage(All_data)\n",
    "\n",
    "n_train =len(All_Y)\n",
    "\n",
    "All_train =  All_data[:n_train] \n",
    "All_test = All_data[n_train:] \n",
    "print('All_train size',All_train.shape,'All_test size',All_test.shape)\n",
    "del All_data\n",
    "\n",
    "X_Train = All_train\n",
    "y_Train = All_Y\n",
    "X_Test = All_test\n",
    "print('--------------------------TrainDataset--------------------------')\n",
    "print('xtrain size is : {}'.format(X_Train.shape),'ytrain size is : {}'.format(y_Train.shape))\n",
    "print('--------------------------TrainDataset--------------------------')\n",
    "print('xtest size is : {}'.format(X_Test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_train = X_Train.shape[0]\n",
    "n_test = X_Test.shape[0]\n",
    "SEED = 42\n",
    "NFOLDS = 5\n",
    "kf = KFold(n_splits= NFOLDS,random_state=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=101, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=101\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.5, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.5\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.1, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.1\n",
      "[LightGBM] [Warning] bagging_freq is set=0, subsample_freq=0 will be ignored. Current value: bagging_freq=0\n",
      "[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=101, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=101\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.5, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.5\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.1, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.1\n",
      "[LightGBM] [Warning] bagging_freq is set=0, subsample_freq=0 will be ignored. Current value: bagging_freq=0\n",
      "[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=101, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=101\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.5, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.5\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.1, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.1\n",
      "[LightGBM] [Warning] bagging_freq is set=0, subsample_freq=0 will be ignored. Current value: bagging_freq=0\n",
      "[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=101, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=101\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.5, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.5\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.1, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.1\n",
      "[LightGBM] [Warning] bagging_freq is set=0, subsample_freq=0 will be ignored. Current value: bagging_freq=0\n",
      "[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=101, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=101\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.5, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.5\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.1, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.1\n",
      "[LightGBM] [Warning] bagging_freq is set=0, subsample_freq=0 will be ignored. Current value: bagging_freq=0\n",
      "Wall time: 2min 37s\n",
      "lgb single AUC score: 0.7651\n"
     ]
    }
   ],
   "source": [
    "lgb = SklearnHelper(clf=LGBMClassifier,seed=SEED,params=lgb_params)\n",
    "%time lgb_oof_train, lgb_oof_test = get_oof(lgb, X_Train, y_Train, X_Test)\n",
    "print(\"lgb single AUC score: %.4f\" % roc_auc_score(y_Train,lgb.predict_proba(X_Train)[:,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 10min 14s\n",
      "CatB single AUC score: 0.7557\n"
     ]
    }
   ],
   "source": [
    "CatB = SklearnHelper(clf=CatBoostClassifier,seed=SEED,params=CatB_params)\n",
    "%time CatB_oof_train, CatB_oof_test = get_oof(CatB, X_Train, y_Train, X_Test)\n",
    "print(\"CatB single AUC score: %.4f\" % roc_auc_score(y_Train,CatB.predict_proba(X_Train)[:,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1h 2min 9s\n",
      "rf single AUC score: 0.8051\n"
     ]
    }
   ],
   "source": [
    "rf = SklearnHelper(clf=RandomForestClassifier,seed=SEED,params=rf_params)\n",
    "%time rf_oof_train, rf_oof_test = get_oof(rf, X_Train, y_Train, X_Test)\n",
    "print(\"rf single AUC score: %.4f\" % roc_auc_score(y_Train,rf.predict_proba(X_Train)[:,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2归一化 MLP训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage of dataframe is 793.46 MB\n",
      "Memory usage after optimization is: 217.44 MB\n",
      "Decreased by 72.6%\n",
      "All_train size (800000, 104) All_test size (200000, 104)\n"
     ]
    }
   ],
   "source": [
    "All_data = pd.read_csv(datapath + 'All_data_NewStep10.csv',encoding='utf-8',delimiter=',')\n",
    "All_Y = pd.read_csv(datapath + 'Y_Final.csv',encoding='utf-8',delimiter=',')\n",
    "All_data = All_data.drop(['id','isDefault'],axis=1)\n",
    "All_data = reduce_mem_usage(All_data)\n",
    "\n",
    "n_train =len(All_Y)\n",
    "\n",
    "##对数据进行归一化\n",
    "minmax_scale = MinMaxScaler(feature_range=(0, 1))\n",
    "All_data = minmax_scale.fit_transform(All_data)\n",
    "\n",
    "All_train =  pd.DataFrame(All_data[:n_train]) \n",
    "All_test = pd.DataFrame(All_data[n_train:])\n",
    "print('All_train size',All_train.shape,'All_test size',All_test.shape)\n",
    "del All_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_params={\n",
    "'hidden_layer_sizes':(128,64,32,16),                   \n",
    "'activation':'relu',                   \n",
    "'solver':'adam',\n",
    "'alpha':0.0001,\n",
    "'batch_size':200,\n",
    "'learning_rate':'adaptive',\n",
    "'learning_rate_init':0.001,\n",
    "'max_iter':5,\n",
    "'early_stopping':True,\n",
    "'n_iter_no_change':10,\n",
    "'beta_1':0.9,\n",
    "'beta_2':0.999,\n",
    "'verbose':1,\n",
    "'epsilon':1e-8}\n",
    "\n",
    "mlp = SklearnHelper(clf=MLPClassifier,seed=SEED,params=mlp_params)\n",
    "%time mlp_oof_train, mlp_oof_test = get_oof(mlp, X_Train, y_Train, X_Test)\n",
    "print(\"lgb single AUC score: %.4f\" % roc_auc_score(y_Train,mlp.predict_proba(X_Train)[:,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(datapath +'lgb_oof_train.npy',lgb_oof_train)\n",
    "np.save(datapath +'lgb_oof_test.npy',lgb_oof_test)\n",
    "np.save(datapath +'CatB_oof_train.npy',CatB_oof_train)\n",
    "np.save(datapath +'CatB_oof_test.npy',CatB_oof_test)\n",
    "np.save(datapath +'rf_oof_train.npy',rf_oof_train)\n",
    "np.save(datapath +'rf_oof_test.npy',rf_oof_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 开始训练Stacking模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LGBMClassifier</th>\n",
       "      <th>CatBoostClassifier</th>\n",
       "      <th>RandomForestClassifier</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.261008</td>\n",
       "      <td>0.277994</td>\n",
       "      <td>0.295534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.360507</td>\n",
       "      <td>0.244783</td>\n",
       "      <td>0.307828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.400846</td>\n",
       "      <td>0.414225</td>\n",
       "      <td>0.410436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.050994</td>\n",
       "      <td>0.063231</td>\n",
       "      <td>0.055297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.425535</td>\n",
       "      <td>0.370200</td>\n",
       "      <td>0.339742</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   LGBMClassifier  CatBoostClassifier  RandomForestClassifier\n",
       "0        0.261008            0.277994                0.295534\n",
       "1        0.360507            0.244783                0.307828\n",
       "2        0.400846            0.414225                0.410436\n",
       "3        0.050994            0.063231                0.055297\n",
       "4        0.425535            0.370200                0.339742"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_predictions_train = pd.DataFrame({'LGBMClassifier':lgb_oof_train.ravel(),\n",
    "                                      'CatBoostClassifier':CatB_oof_train.ravel(),\n",
    "                                      'RandomForestClassifier':rf_oof_train.ravel()})\n",
    "base_predictions_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "X2_Train = np.concatenate((lgb_oof_train,CatB_oof_train,rf_oof_train),axis=1)\n",
    "X2_Test = np.concatenate((lgb_oof_test,CatB_oof_test,rf_oof_test),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR stacking AUC score: 0.7415\n"
     ]
    }
   ],
   "source": [
    "lr2 = LGBMClassifier().fit(X2_Train, y_Train)\n",
    "#LogisticRegression,LGBMClassifier\n",
    "print(\"LR stacking AUC score: %.4f\" % roc_auc_score(y_Train,lr2.predict_proba(X2_Train)[:,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "Test_data =pd.read_csv('testA.csv',encoding='utf-8',delimiter=',' )\n",
    "Test_data['isDefault']=lr2.predict_proba(X2_Test)[:,1]\n",
    "Test_data[['id','isDefault']].to_csv('FR_waittest2.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
